\chapter{Basic CNN Architecture}
\label{ch:basic_cnn_arch}%

A \textbf{Convolutional Neural Network (CNN)} is composed of a sequence of specialized layers designed to efficiently process high-dimensional inputs such as images.
\\
Unlike fully connected networks, CNNs exploit the spatial structure of data to extract hierarchical features while keeping the number of learnable parameters relatively low \cite{goodfellow2016deep}.
\\
In this section, we explore the main building blocks of a CNN: \textbf{convolutional layers}, \textbf{pooling layers}, \textbf{activation functions}, \textbf{batch normalization}, and \textbf{fully connected layers}.

\section{Convolutional Layers}

The \textbf{convolutional layer} is the core component of a CNN. It applies a set of learnable \textbf{kernels} (or filters) to the input image or feature map, extracting patterns such as \textbf{edges}, \textbf{textures}, or \textbf{shapes} \cite{goodfellow2016deep}.
\\
Each kernel slides across the input with a given stride and computes a dot product between the kernel weights and the local region of the input, producing a feature map.
This process allows the network to detect local structures while sharing parameters across the entire image, dramatically reducing the total number of learnable weights compared to fully connected architectures.
\\
Key hyperparameters of a convolutional layer:

\begin{itemize}
    \item \textbf{Kernel size}: determines the size of the receptive field, e.g. $3 \times 3$ or $5 \times 5$.
    \item \textbf{Stride} : controls how far the kernel moves at each step; larger strides reduce the spatial resolution.
    \item \textbf{Padding}: determines how to handle edges.

    \begin{itemize}
        \item Valid padding: no padding, output shrinks.
        \item Same padding: zero-padding to maintain input dimensions.
    \end{itemize}
\end{itemize}

For example, a convolutional layer with 64 filters of size $3 \times 3$ on a $224 \times 224 \times 3$ input produces 64 feature maps, each detecting a different learned pattern.

\section{Pooling Layers}

Pooling layers are used to downsample the spatial dimensions of feature maps while retaining the most important information \cite{goodfellow2016deep}.
This reduces the computational cost and improves the model’s ability to generalize.
\\
The two most common types are:

\begin{itemize}
    \item \textbf{Max} pooling: selects the maximum value within a local region.
    \item \textbf{Average} pooling: computes the average value within the region.
\end{itemize}

For instance, applying a $2 \times 2$ max pooling operation with stride 2 on a $32 \times 32$ feature map reduces the output to $16 \times 16$.
\\
Pooling layers offer two main advantages:
\begin{enumerate}
    \item Translation invariance: small shifts in the input do not significantly affect the feature maps.
    \item Dimensionality reduction: fewer parameters and less computation, reducing the risk of overfitting.
\end{enumerate}

\section{Activation Functions}

Activation functions introduce \textbf{non-linearity} into the network, enabling CNNs to learn complex patterns that cannot be captured by linear transformations alone.
Without them, stacking multiple layers would result in a model equivalent to a single linear transformation.
\\
Common activation functions include:

\begin{itemize}
    \item \textbf{ReLU} (Rectified Linear Unit):
    
    \begin{align}
        \text{ReLU}(x) = \max(0, x)
    \end{align}
    
    Computationally efficient and alleviates the vanishing gradient problem.

    \item \textbf{Leaky} ReLU: allows a small, non-zero gradient for negative inputs, addressing “dying ReLU” neurons.

    \item \textbf{ELU} (Exponential Linear Unit): smooths the activation curve and sometimes improves learning speed.
    
\end{itemize}

According to Li et al. \cite{li2021survey}, ReLU and its variants are by far the most widely used activation functions in modern CNNs because of their simplicity and effectiveness.

\section{Batch Normalization}

\textbf{Batch Normalization (BatchNorm)} normalizes the activations of a layer across each mini-batch, ensuring they have zero mean and unit variance \cite{li2021survey}.
This technique has several benefits:
\begin{itemize}
    \item Faster convergence during training.
    \item Stabilized learning, reducing sensitivity to weight initialization.
    \item Regularization effect, improving generalization and reducing overfitting.
\end{itemize}

Introduced in 2015, BatchNorm is now a standard component in most modern CNNs and is typically placed after the convolution but before the activation function.

\section{Fully Connected Layers}

After several convolutional and pooling layers, the extracted features are flattened and passed to one or more fully connected (dense) layers.
These layers integrate the learned features and perform the final classification or regression task.
\\
Each neuron in a fully connected layer connects to all activations from the previous layer. The final layer often uses a softmax activation to produce a probability distribution over output classes.
\\
For example, in the ImageNet classification task, the last fully connected layer outputs a 1,000-dimensional vector, where each value corresponds to the predicted probability of a specific class \cite{goodfellow2016deep}.

\section{Summary of the Data Flow}

A typical CNN processes data as follows:
\begin{enumerate}
    \item Input Layer: raw image data.
    \item Convolutional Layers: extract local patterns such as edges and textures.
    \item Pooling Layers: reduce spatial dimensions while preserving essential information.
    \item Batch Normalization \& Activations: stabilize and accelerate learning.
    \item Fully Connected Layers: combine extracted features into final predictions.
    \item Output Layer: provides class probabilities or regression values, depending on the task.
\end{enumerate}