\chapter*{Introduction}

Deep learning has revolutionized modern machine learning by enabling neural networks to automatically extract useful representations from raw data.  
Among the architectures that made this progress possible, \textit{Convolutional Neural Networks} (CNNs) stand out as one of the most influential.  
Originally developed for computer vision, CNNs have also achieved remarkable success in speech recognition, time-series forecasting and natural language processing.  

Unlike traditional fully connected networks, CNNs are designed to exploit the \textit{spatial and structural properties of data}.  
They achieve this by using the mathematical operation of convolution, which processes information locally and hierarchically.  
This principle reflects the idea that nearby elements of the input (such as adjacent pixels in an image) are highly correlated and should be analyzed together.  
As a result, CNNs learn models that are more efficient, require fewer parameters and generalize better than dense architectures.  

Conceptually, CNNs bring together mathematical insights from signal processing and biological inspiration from neuroscience.  
The notion of local receptive fields in the human visual cortex, described in the pioneering experiments of Hubel and Wiesel, 1968 \cite{hubel1968}, directly influenced the design of convolutional layers.   

In this work, we address the mathematical definition of convolution and the motivation for its use in neural networks, analyze the role of pooling and its modern alternatives and discuss several important variants of the convolution operator.  
We then review applications of convolution across different types of data, from one-dimensional signals to video and volumetric analysis and examine the neuroscientific principles that inspired CNNs.